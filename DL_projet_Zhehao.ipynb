{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOO2IUTOLvC8kA1r+xU1NPE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhehao-CUI/DL_projet/blob/main/DL_projet_Zhehao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import\n",
        "import math, time, random, os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 2. Reproducibility + Device\n",
        "\"\"\"\n",
        "Setting the seed for reproducibility.\n",
        "Setting the device to 'cuda' if available, otherwise 'cpu'.\n",
        "\"\"\"\n",
        "seed = 1228\n",
        "random.seed(seed) # set the seed for python's build-in random module\n",
        "torch.manual_seed(seed) # set the seed for pythorch's random number generator on the CPU\n",
        "torch.cuda.manual_seed_all(seed) # set the seed for pytorch CUDA random number generator for all GPUs\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # choose the device\n",
        "print(\"device:\", device)\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaSi3baWUSUn",
        "outputId": "c01a9380-dee5-4f42-f752-da931036bf70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Download the dataset\n",
        "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt # ! -> run a shell command to download the text file from site -o: output of command is \"input.text\"\n",
        "text = open(\"input.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "print(\"dataset chars:\", len(text))\n",
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb0Xd_BuXBm0",
        "outputId": "05ea4650-b22c-4cf8-804e-cb5c948d111c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset chars: 1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Char tokenizer\n",
        "\"\"\"\n",
        "creats a tokenizer to tokenize the text into characters\n",
        "each character is mapped to an integer ID using stoi\n",
        "for a given character, stoi returns the corresponding integer ID and encode() produces a 1D tensor of those IDs\n",
        "\"\"\"\n",
        "chars = sorted(list(set(text))) # collect all unique characters in the dataset, converts the set to a list and sort the list\n",
        "print(chars[:10])\n",
        "vocab_size = len(chars) # number of unique characters\n",
        "stoi = {ch:i for i,ch in enumerate(chars)} # map each char (ch) to an integer index i\n",
        "itos = {i:ch for ch,i in stoi.items()} # inverse mapping\n",
        "\n",
        "def encode(s: str) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  encodes a string into a tensor of integers\n",
        "  \"\"\"\n",
        "  return torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
        "\n",
        "def decode(ids) -> str:\n",
        "  \"\"\"\n",
        "  decodes a tensor of integers into a string\n",
        "  \"\"\"\n",
        "  if isinstance(ids, torch.Tensor):\n",
        "      ids = ids.tolist()\n",
        "  return \"\".join(itos[i] for i in ids)\n",
        "\n",
        "data = encode(text) # convert the entire dataset text into a 1D tensor of char id\n",
        "n = int(0.9 * len(data)) # split the training and text data (90% vs. 10%)\n",
        "train_data = data[:n]\n",
        "val_data   = data[n:]\n",
        "print(\"vocab_size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jibFmL0pX6Yn",
        "outputId": "8d49f6cf-b0df-4992-bd64-a372ef23db11"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3']\n",
            "vocab_size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. Training hyperparams\n",
        "\"\"\"\n",
        "define the hyperparameters for training the model\n",
        "\"\"\"\n",
        "block_size    = 128   # context length : the model looks at 128 chars at a time to predict the next char\n",
        "batch_size    = 64    # number of sequence processed (64 X 128 = 8192 tokens) processed in parallel per training step\n",
        "max_iters     = 12000 # how many times we update those weight during training\n",
        "eval_interval = 500   # every 500 steps, we evaluate the train/val loss averages\n",
        "eval_iters    = 100   # when evaluating, average loss over 100 batches to reduce noise\n",
        "\n",
        "learning_rate = 3e-4  # learning rate (how big each parameter update step is)\n",
        "\n",
        "\n",
        "# model size : 12 layers, 8 heads, 768 embd\n",
        "n_embd  = 768\n",
        "n_head  = 8\n",
        "n_layer = 12\n",
        "dropout = 0.1\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "FzJDfvj5bYpO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Batch sampling (sliding window)\n",
        "\"\"\"\n",
        "creates a batch of batch_size sequences, and each sequence has length block_size = 128.\n",
        "\"\"\"\n",
        "def get_batch(split: str):\n",
        "    src = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(0, len(src) - block_size - 1, (batch_size,)) # pick random starting position\n",
        "    x = torch.stack([src[i:i+block_size] for i in ix])          # X (batch_size, block_size) = input\n",
        "    y = torch.stack([src[i+1:i+block_size+1] for i in ix])      # Y (batch_size, block_size) = target sequence (one character ahead)\n",
        "                                                                # -> given the characters up to position t, predict the next character\n",
        "    return x.to(device), y.to(device)"
      ],
      "metadata": {
        "id": "YmkOC09JfN8-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. GPT model\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.n_head = n_head\n",
        "        self.head_dim = n_embd // n_head # split the embedding dimension C=n_embd into n_head heads\n",
        "\n",
        "        self.qkv  = nn.Linear(n_embd, 3 * n_embd, bias=False) # single linear layer that output queries, keys and values (batch_size, block_size, n_embd)\n",
        "        self.proj = nn.Linear(n_embd, n_embd, bias=False) #  output projection layer in the attention module\n",
        "        self.dropout = nn.Dropout(dropout) # set a fraction of the elements to 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward pass for the CausalSelfAttention module\n",
        "        input x: (batch_size, block_size, n_embd) -> token embedding x\n",
        "        output y: (batch_size, block_size, n_embd) -> transformed token embedding\n",
        "        \"\"\"\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.split(C, dim=-1) #split the last dimension into 3 chunks of size C\n",
        "\n",
        "        #reshape into multiple heads\n",
        "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (batch_size, n_head, T, head_dim)\n",
        "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # compute causal attention\n",
        "        y = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            attn_mask=None,\n",
        "            dropout_p=self.dropout.p if self.training else 0.0,\n",
        "            is_causal=True # token at position t can only attend to positions <= t\n",
        "        )  # (B,nh,T,hd)\n",
        "        # merge head back\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)          # (B,T,C)\n",
        "        y = self.dropout(self.proj(y)) # output projection + dropout\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    feed-forward network with two linear layers and a GELU activation function\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(n_embd, 4 * n_embd) # first linear layer expands the dimension: C to 4C\n",
        "        self.fc2 = nn.Linear(4 * n_embd, n_embd) # second linear layer  projects back down: 4C to C\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.fc2(F.gelu(self.fc1(x)))) # x -> fc1 -> GELU -> fc2 -> dropout\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder block : LayerNorm -> Causal self-attention/MLP -> residual add\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embd, n_head, dropout):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = CausalSelfAttention(n_embd, n_head, dropout)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = MLP(n_embd, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # required form:\n",
        "        x = x + self.attn(self.ln1(x)) # residual add : original x + the attention \"update\"\n",
        "        x = x + self.mlp(self.ln2(x)) # residual add : original x + MLP update\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, n_embd, n_head, n_layer, dropout):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.wte = nn.Embedding(vocab_size, n_embd)      # token embeddings (WTE) : map each token ID to a vector of length n_embd\n",
        "        self.wpe = nn.Embedding(block_size, n_embd)      # position embeddings (WPE) : give each position its own embedding vector\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList([Block(n_embd, n_head, dropout) for _ in range(n_layer)]) # creates n_layer identical Transformer decoder blocks.\n",
        "        self.ln_f = nn.LayerNorm(n_embd)                 # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False) # converts hidden state vecotr into logit over vocabulary\n",
        "\n",
        "        self.apply(self._init_weights) # initialize weights with a normal distribution\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"\n",
        "        forward pass for the GPT model :\n",
        "          1. predicted logits for the next token at every position\n",
        "          2. training loss\n",
        "        \"\"\"\n",
        "        B, T = idx.shape\n",
        "        assert T <= self.block_size, \"Sequence length exceeds block_size\"\n",
        "\n",
        "        pos = torch.arange(0, T, device=idx.device) # creat positions indices\n",
        "        tok_emb = self.wte(idx)              # WTE(idx) (B,T) -> (B,T,C)\n",
        "        pos_emb = self.wpe(pos)              # WPE(pos) (T,)  -> (T,C)\n",
        "        x = self.drop(tok_emb + pos_emb)     # Dropout(tok_emb + pos_emb) # (B,T,C)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x) # pass x through n_layer decoder blocks\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x) # unnormalized score for each vocabulary token at each positions\n",
        "\n",
        "        loss = None # computing loss\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, greedy=False):\n",
        "        \"\"\"\n",
        "        generate new text tokens\n",
        "        \"\"\"\n",
        "        self.eval() # put the model to the evaluation mode\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:] # (B, â‰¤block_size)\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / max(temperature, 1e-8) # (B, V)\n",
        "\n",
        "            if top_k is not None:\n",
        "                \"\"\"\n",
        "                keeps only the top k highest logits per batch row\n",
        "                \"\"\"\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1) # raw scores -> probability\n",
        "            # choose the next token\n",
        "            if greedy:\n",
        "                next_id = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "            else:\n",
        "                next_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "IiRiwZqhjq3V"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Init model + optimizer adam\n",
        "model = GPT(vocab_size, block_size, n_embd, n_head, n_layer, dropout).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # REQUIRED\n",
        "\n",
        "# AMP\n",
        "use_amp = (device == \"cuda\")\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in [\"train\", \"val\"]: # for both training and validation set : compute the loss and average\n",
        "        losses = []\n",
        "        for _ in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "                _, loss = model(x, y)\n",
        "            losses.append(loss.item())\n",
        "        out[split] = sum(losses) / len(losses)\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "bd973yDnoJhm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 9. Train\n",
        "t0 = time.time()\n",
        "for it in range(1, max_iters + 1):\n",
        "    x, y = get_batch(\"train\")\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True) # clear old gradient\n",
        "    with torch.amp.autocast(\"cuda\", enabled=use_amp): #foward pass\n",
        "        _, loss = model(x, y)\n",
        "\n",
        "    scaler.scale(loss).backward() # backprop\n",
        "    scaler.step(optimizer)   # actually updates parameters\n",
        "    scaler.update()          # updates scaler for next iteration\n",
        "\n",
        "    if it % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        val_ppl = math.exp(losses[\"val\"])\n",
        "        print(f\"iter {it}/{max_iters} | train {losses['train']:.4f} | val {losses['val']:.4f} | ppl {val_ppl:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoL89hGyo4y1",
        "outputId": "72edebf4-3d0d-4b78-9d3c-7083d8374372"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 500/12000 | train 2.0636 | val 2.1295 | ppl 8.41\n",
            "iter 1000/12000 | train 1.6241 | val 1.7974 | ppl 6.03\n",
            "iter 1500/12000 | train 1.4255 | val 1.6289 | ppl 5.10\n",
            "iter 2000/12000 | train 1.3143 | val 1.5742 | ppl 4.83\n",
            "iter 2500/12000 | train 1.2500 | val 1.5217 | ppl 4.58\n",
            "iter 3000/12000 | train 1.1824 | val 1.5093 | ppl 4.52\n",
            "iter 3500/12000 | train 1.1330 | val 1.5292 | ppl 4.61\n",
            "iter 4000/12000 | train 1.0670 | val 1.5245 | ppl 4.59\n",
            "iter 4500/12000 | train 1.0079 | val 1.5529 | ppl 4.72\n",
            "iter 5000/12000 | train 0.9350 | val 1.6171 | ppl 5.04\n",
            "iter 5500/12000 | train 0.8594 | val 1.6398 | ppl 5.15\n",
            "iter 6000/12000 | train 0.7672 | val 1.7129 | ppl 5.55\n",
            "iter 6500/12000 | train 0.6767 | val 1.7675 | ppl 5.86\n",
            "iter 7000/12000 | train 0.5797 | val 1.8740 | ppl 6.51\n",
            "iter 7500/12000 | train 0.5010 | val 2.0120 | ppl 7.48\n",
            "iter 8000/12000 | train 0.4216 | val 2.1060 | ppl 8.22\n",
            "iter 8500/12000 | train 0.3644 | val 2.2090 | ppl 9.11\n",
            "iter 9000/12000 | train 0.3098 | val 2.3424 | ppl 10.41\n",
            "iter 9500/12000 | train 0.2695 | val 2.4320 | ppl 11.38\n",
            "iter 10000/12000 | train 0.2414 | val 2.5614 | ppl 12.95\n",
            "iter 10500/12000 | train 0.2197 | val 2.6526 | ppl 14.19\n",
            "iter 11000/12000 | train 0.2125 | val 2.7487 | ppl 15.62\n",
            "iter 11500/12000 | train 0.1962 | val 2.8260 | ppl 16.88\n",
            "iter 12000/12000 | train 0.1871 | val 2.8679 | ppl 17.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. test\n",
        "seed_str = \"O God, O God!\"\n",
        "context = encode(seed_str).unsqueeze(0).to(device)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(context, max_new_tokens=800, temperature=0.9, top_k=50, greedy=False)\n",
        "print(decode(out[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMbLwjzqxsNe",
        "outputId": "fa3c5c73-95cb-4c5c-b883-1e55fa98bca9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O God, O God! that e'er this tongue\n",
            "But that raised him to the king entertain,\n",
            "This satisfaction made the day of Juliet.\n",
            "\n",
            "JULIET:\n",
            "I will confirm thee to the extremest point.\n",
            "\n",
            "ROMEO:\n",
            "That art thou worthy then well for this action.\n",
            "\n",
            "MERCUTIO:\n",
            "Nay, I'll conjure too.\n",
            "Romeo! humours! madman! passion! lover!\n",
            "Appear thou in the likeness of a sigh:\n",
            "Speak but one rhyme, and I am satisfied;\n",
            "Cry but 'Ay me!' pronounce but 'love' and 'dove;'\n",
            "Speak to my gossip Venus one fair word,\n",
            "One nick-name for her persons.\n",
            "\n",
            "VIRGILIA:\n",
            "No, good madam; I will not over the\n",
            "threshold till my lord return from the wars.\n",
            "\n",
            "VALERIA:\n",
            "Fie, you confine yourself most unreasonably: come, lend you thither.\n",
            "\n",
            "VIRGILIA:\n",
            "\n",
            "MISTRESS OVERDONE:\n",
            "Why, what's that?\n",
            "\n",
            "BUCKINGHAM:\n",
            "Marry, my lord, will not prove it.\n",
            "\n",
            "PARIS:\n",
            "Do not you come to make your sta\n"
          ]
        }
      ]
    }
  ]
}